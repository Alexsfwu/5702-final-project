---
title: "EDAV Fall 2019 Final Project Group 23"
author: "Shaofeng Wu, Chengyou Ju, Mingrui Liu, Yujing Song"
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE)
```

## I. Introduction

This project is created by Shaofeng Wu (sw3428), Chengyou Ju (cj2624), Mingrui Liu (ml4404), Yujing Song (ys3251) for STAT GR5702 Final Project Group 23.

As graduate students who are actively seeking jobs and internships, we come up with the idea that we can choose a topic related to the current employment situation in New York. Therefore, after doing some research, we decide to focus on studying the latest job postings in New York. We want to see, for instance, if there is a relationship between job categories and salaries, what skills are preferred by a particular kind of jobs, or if locations matter while choosing a job.


## II. Data sources

Our data come from https://data.cityofnewyork.us/City-Government/NYC-Jobs/kpav-sd4t, which contains current job postings available on the City of New Yorkâ€™s official jobs site http://www.nyc.gov/html/careers/html/search/search.shtml. 

Mingrui is responsible for looking for and collecting the data. Since the data can be directly exported from the website as a `.csv` file, there is no major obstacle for us when gathering the data. After downloading the data from the website, we use the built-in `read.csv` method to read and store the data as a data frame for future manipulation.

This data frame has 3040 oberservations of 28 variables. Each entry represents a job posting on the website, while the columns represent the information about each job, including Business Title, Salary Range, Job Description, etc.

The only problem about the data is that in the origin `.csv` file, there are plenty of empty entries. Therefore, when we read data from the file and store as a data frame, we fill all these blank entries with NAs.


## III. Data transformation

As mentioned in the previous section, since the dataset is stored in a neat `.csv` file, we only need to read the file using the `read.csv` method and store the data in the data frame `job`. Later, when we need some specific attributes of the job postings dataset, we can use simply `$` or pipes and the `%>%` operator to extract the columns we need.
```{r}
library(dplyr)
library(corpus)
library(ggplot2)
library(magrittr)
library(forcats)
library(tidyverse)
library(tm)
library(htmlTable)
```

```{r}
job <- read.csv("data/raw/NYC_jobs.csv") # Read data from csv file
job[job == ''] <- NA # Set empty slots as NAs

# Remove duplicate jobs 
job <-job[!duplicated(job), ]

# dim(job) # Check the dimension of the data frame
```

## IV. Missing values 

As mentioned above, we fill all empty slots in the original dataset with NAs, and we will keep these NAs in our data frame. Later, depends on our tasks, we will decide whether to drop the NAs or not.
```{r}
library(mi)
x <- missing_data.frame(job)
image(x)
```



## V. Results
This part is mainly contributed by Mingrui.
```{r}
categoryList <- job %>%
  filter(!is.na(Job.Category))%>%
  select(Job.Category, Job.ID) %>%
  mutate(Job.Category = as.character(Job.Category),
         Job.Category = str_split(Job.Category, ",|&|,&"))

popular_category <-
  as.data.frame(unlist(categoryList["Job.Category"],use.names=FALSE))%>%
  set_colnames("Category")%>%
  mutate(Category = trimws(Category,"both")) %>%
  filter(!is.na(Category))%>%
  filter(Category !="")%>%
  filter(is.character(Category ))  %>%
  group_by(Category) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice(1:25)
  
ggplot(popular_category,aes(x = fct_reorder(Category,count), y = count))+
  geom_col(color = "black", fill = "orange")+
  ggtitle("Job Count by Category") +
  labs(x = "Category", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  coord_flip()
```
```{r}
job <- job %>%
  mutate(salary = Salary.Range.From+(Salary.Range.To-Salary.Range.From)/2)

Annual = job[job$Salary.Frequency=="Annual",]
ggplot(Annual, aes(salary)) +
  geom_histogram(bins = 40, color = "black", fill = "orange") +
  ggtitle("Salary Distribution (Annual)") + 
  labs(x = "Salary Range", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5))
```
```{r}
Daily = job[job$Salary.Frequency=="Daily",]
ggplot(data = Daily, aes(Daily$salary)) +
  geom_histogram(bins = 40, color = "black", fill = "orange") +
  ggtitle("Salary Distribution (Daily)") + 
  labs(x = "Salary Range", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5))
```
```{r}
Hourly = job[job$Salary.Frequency=="Hourly",]
ggplot(data = Hourly, aes(Hourly$salary)) +
  geom_histogram(bins = 40, color = "black", fill = "orange") +
  ggtitle("Salary Distribution (Hourly)") + 
  labs(x = "Salary Range", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5))
```

Meanwhile, we also want to study the minimum qualification requirements and preferred skills for the available jobs in NYC. We want to find if there are any patterns in these two columns or if we can extract any useful information from them. In order to illustrate our findings graphfically, we decide to use Word Clouds to show the most frequent words in these texts.

So what is Word Clouds? Word Clouds is visual representations of text data. They are useful for quickly perceiving the most prominent terms, which makes them widely used in media and well understood by the public.

A Word Cloud is a collection of words depicted in different sizes. The bigger and bolder the word appears, the greater frequency within a given text and the more important it is.

In order to extract meaningful vocabularies from the text descriptions, we take advantage of the text mining package `tm` in R. This package is based on the ideas of Natural Language Processing (NLP). It have methods that can tranform all words to lowercases, remove words that are uninformative in Enlighs such as "a" and "the", and get rid of whitespaces and punctuations.

After these manipulations on the text data, we can create a new data frame of word frequencies. We can also sort it by frequency and find out the most frequent words under minimum qualification requirements and preferred skills for all jobs or for any particular category of jobs that we are interested in.

The table below demonstrates the 20 most frequent words in Minimum Qual Requirements among all jobs in out dataset.
```{r}
job_docs <- VCorpus(VectorSource(job)) # Whole dataset
# inspect(job_docs)
job_mini_req <- VCorpus(VectorSource(job$Minimum.Qual.Requirements)) # Minimum Qual Requirements
# inspect(job_mini_req)
job_pref_skil <- VCorpus(VectorSource(job$Preferred.Skills)) # Preferred Skills
# inspect(job_pref_skil)
```
```{r}
# Tech Jobs
tech_jobs <- subset(job, Job.Category == "Technology, Data & Innovation")
# dim(tech_jobs) # There should be 28 jobs related to technology
job_tech_mini_req <- VCorpus(VectorSource(tech_jobs$Minimum.Qual.Requirements))
# inspect(job_tech_mini_req)
job_tech_pref_skil <- VCorpus(VectorSource(tech_jobs$Preferred.Skills))
# inspect(job_tech_pref_skil)
```
```{r}
# All Jobs cross Minimum Qual Requirements
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_mini_req <- tm_map(job_mini_req, toSpace, "/")
# job_mini_req <- tm_map(job_mini_req, toSpace, "@")
# job_mini_req <- tm_map(job_mini_req, toSpace, "\\|")
job_mini_req <- tm_map(job_mini_req, content_transformer(tolower))
job_mini_req <- tm_map(job_mini_req, removeNumbers)
job_mini_req <- tm_map(job_mini_req, removeWords, stopwords("english"))
job_mini_req <- tm_map(job_mini_req, removeWords, c("the", "one", "two", "for", "must")) 
job_mini_req <- tm_map(job_mini_req, removePunctuation)
job_mini_req <- tm_map(job_mini_req, stripWhitespace)
# job_mini_req <- tm_map(job_mini_req, stemDocument)
```
```{r}
mini_req_matrix <- TermDocumentMatrix(job_mini_req)
mini_freq_m <- as.matrix(mini_req_matrix)
mini_freq_v <- sort(rowSums(mini_freq_m), decreasing=TRUE)
mini_freq <- data.frame(word = names(mini_freq_v), freq=mini_freq_v)
# head(mini_freq, 20)
htmlTable(head(mini_freq, 20), caption="Minimum Qual Requirements in All Jobs Word Frequency", header=c("Word", "Frequency"), rnames=FALSE)
```
```{r}
library(wordcloud2)
wordcloud2(data = mini_freq, color = 'random-light', backgroundColor = "black")
```

```{r}
# All Jobs cross Preferred Skills
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_pref_skil <- tm_map(job_pref_skil, toSpace, "/")
# job_pref_skil <- tm_map(job_pref_skil, toSpace, "@")
# job_pref_skil <- tm_map(job_pref_skil, toSpace, "\\|")
job_pref_skil <- tm_map(job_pref_skil, content_transformer(tolower))
job_pref_skil <- tm_map(job_pref_skil, removeNumbers)
job_pref_skil <- tm_map(job_pref_skil, removeWords, stopwords("english"))
job_pref_skil <- tm_map(job_pref_skil, removeWords, c("the", "one", "two", "for", "must")) 
job_pref_skil <- tm_map(job_pref_skil, removePunctuation)
job_pref_skil <- tm_map(job_pref_skil, stripWhitespace)
# job_pref_skil <- tm_map(job_pref_skil, stemDocument)
```
The table below demonstrates the 20 most frequent words in Preferred Skills among all jobs in out dataset.
```{r}
pref_skil_matrix <- TermDocumentMatrix(job_pref_skil)
pref_freq_m <- as.matrix(pref_skil_matrix)
pref_freq_v <- sort(rowSums(pref_freq_m), decreasing=TRUE)
pref_freq <- data.frame(word = names(pref_freq_v), freq=pref_freq_v)
pref_freq <- pref_freq[-1,]
# head(pref_freq, 20)
htmlTable(head(pref_freq, 20), caption="Preferred Skills in All Jobs Word Frequency", header=c("Word", "Frequency"), rnames=FALSE)
```
```{r}
wordcloud2(data = pref_freq, color = 'random-light', backgroundColor = "black")
```

```{r}
# Tech Jobs cross Minimum Qual Requirements
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_tech_mini_req <- tm_map(job_tech_mini_req, toSpace, "/")
# job_tech_mini_req <- tm_map(job_tech_mini_req, toSpace, "@")
# job_tech_mini_req <- tm_map(job_tech_mini_req, toSpace, "\\|")
job_tech_mini_req <- tm_map(job_tech_mini_req, content_transformer(tolower))
job_tech_mini_req <- tm_map(job_tech_mini_req, removeNumbers)
job_tech_mini_req <- tm_map(job_tech_mini_req, removeWords, stopwords("english"))
job_tech_mini_req <- tm_map(job_tech_mini_req, removeWords, c("the", "one", "two", "for", "must")) 
job_tech_mini_req <- tm_map(job_tech_mini_req, removePunctuation)
job_tech_mini_req <- tm_map(job_tech_mini_req, stripWhitespace)
# job_tech_mini_req <- tm_map(job_tech_mini_req, stemDocument)
```
The table below demonstrates the 20 most frequent words in Minimum Qual Requirements among all jobs in out dataset.
```{r}
tech_mini_matrix <- TermDocumentMatrix(job_tech_mini_req)
tech_mini_freq_m <- as.matrix(tech_mini_matrix)
tech_mini_freq_v <- sort(rowSums(tech_mini_freq_m), decreasing=TRUE)
tech_mini_freq <- data.frame(word = names(tech_mini_freq_v), freq=tech_mini_freq_v)
# head(tech_mini_freq, 20)
htmlTable(head(tech_mini_freq, 20), caption="Minimum Qual Requirements in Technology Related Jobs Word Frequency", header=c("Word", "Frequency"), rnames=FALSE)
```
```{r}
wordcloud2(data = tech_mini_freq, color = 'random-light', backgroundColor = "black")
```

```{r}
# Tech Jobs cross Preferred Skills
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, toSpace, "/")
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, toSpace, "@")
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, toSpace, "\\|")
job_tech_pref_skil <- tm_map(job_tech_pref_skil, content_transformer(tolower))
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removeNumbers)
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removeWords, stopwords("english"))
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removeWords, c("the", "one", "two", "for", "must")) 
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removePunctuation)
job_tech_pref_skil <- tm_map(job_tech_pref_skil, stripWhitespace)
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, stemDocument)
```
```{r}
tech_pref_matrix <- TermDocumentMatrix(job_tech_pref_skil)
tech_pref_freq_m <- as.matrix(tech_pref_matrix)
tech_pref_freq_v <- sort(rowSums(tech_pref_freq_m), decreasing=TRUE)
tech_pref_freq <- data.frame(word = names(tech_pref_freq_v), freq=tech_pref_freq_v)
tech_pref_freq <- tech_pref_freq[-1,]
# head(tech_pref_freq, 20)
htmlTable(head(tech_pref_freq, 20), caption="Preferred Skills in Technology Related Jobs Word Frequency", header=c("Word", "Frequency"), rnames=FALSE)
```
```{r}
wordcloud2(data = tech_pref_freq, color = 'random-light', backgroundColor = "black")
```


```{r}
tech_job = job[job$Job.Category == 'Technology, Data & Innovation', ]

tech_job %>%
  filter(!is.na(tech_job$Full.Time.Part.Time.indicator)) %>%
  ggplot(aes(Full.Time.Part.Time.indicator)) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Full time and part time")
```


```{r}
tech_job %>%
  filter(!is.na(tech_job$Salary.Frequency)) %>%
  ggplot(aes(Salary.Frequency)) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Salary frequency")
```


```{r}
tech_job %>%
  filter(!is.na(tech_job$Civil.Service.Title)) %>%
  ggplot(aes(fct_rev(fct_infreq(Civil.Service.Title)))) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Civil service title") + 
  coord_flip()
```


```{r}
tech_job %>%
  filter(!is.na(tech_job$Work.Location)) %>%
  ggplot(aes(fct_rev(fct_infreq(Work.Location)))) + 
  geom_bar(stat = 'count', fill = 'lightblue') +
  labs(x = "Work location") + 
  coord_flip()
```


```{r}
tech_job %>%
  group_by(Civil.Service.Title) %>%
  summarise(mean_salary = mean(salary)) %>%
  ggplot(aes(x = reorder(Civil.Service.Title, mean_salary), y = mean_salary)) +
  geom_col(fill = 'lightblue') + 
  labs(x = 'Civil service title', 
       y = 'Average annual salary') + 
  coord_flip()
```


## VI. Interactive component
```{r}
# for(i in 1:nrow(job)){
#  result = geocode( as.character(job$Work.Location[i]) , source = 'google', output = 'latlona')
#  if(!is.na(result[1])){
#    job$longitude[i] = as.numeric(result[1])
#    job$latitude[i] = as.numeric(result[2])
#    job$address[i] = as.character(result[3])
#  }else{
#    job$latitude[i] = NA
#    job$longitude[i] = NA
#    job$address[i] = NA
#  }
# }

#write_csv(job, 'job_with_lat_long.csv')
```




```{r}
library(leaflet)
job_with_location = read_csv('job_with_lat_long.csv')%>%
  filter(!is.na(latitude))%>%
  filter(longitude < -74.0060 + 1) %>%
  filter(longitude > -74.0060 - 1) %>%
  filter(latitude < 40.7128 + 0.8) %>%
   filter(latitude > 40.7128 - 0.8)
  
#removing values for which there were no coordinates
high_salary_geocodes = job_with_location%>%
  filter(Annual_salary>mean(Annual_salary))
low_salary_geocodes = job_with_location%>%
  filter(Annual_salary<mean(Annual_salary))

#add custom color column
high_salary_geocodes$color = rep('Blue', nrow(high_salary_geocodes))
low_salary_geocodes$color = rep('Red', nrow(low_salary_geocodes))

salary_geocodes = rbind(high_salary_geocodes, low_salary_geocodes)

salary_geocodes %>% 
  leaflet(data = .) %>% 
  addProviderTiles(providers$Esri) %>%
  setView(lat = 40.7128, lng = -74.0060, zoom = 11) %>%
  addTiles() %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 3, 
                   color = ~color, 
                   popup = ~paste("<strong>Angency</strong>", salary_geocodes$Agency,
                                  "<br /><strong>Job Title:</strong>", salary_geocodes$Business.Title,
                                  "<br /><strong>Annual Salary:</strong>","$",salary_geocodes$Annual_salary,
                                  "<br /><strong>Category:</strong>",salary_geocodes$Job.Category))


```

## VII. Conclusion

b) How do you plan to divide up the work? (Grading is on a group basis. The point of asking is to encourage you to think about this.)
To

    Chengyou may will work on the topic of salary vs. agency, and he may also use the word cloud to explore the relationship between these two variables. 
    
    Shaofeng will work on the data cleaning and mabipulation, and he will try to find connections among NAs or may come up with interesting topics when doing the data cleaning. 
    
    Yujing will investigate on the topic of salary vs. regions, and she also wants to draw a map to show the salary's region distribution.
    
    Mingrui will focus on the topic of salary vs. job category. Using different groups of job category to classify different levels of salaries may be something could be explored.
    
### 1.2 The Problem Statement

List three questions that you hope you will be able to answer from your research.

  Main topic: What factors maybe influence salary?

    a) The relationship between salary vs. regions.

    b) The relationship between salary vs. job category.

    c) The relationship between salary vs. agency.



# 2. Data Exploration

### 2.1 The Data Discription





### 2.2 Short Summary of Initial Investigations

From those three plots above, we can have the following obeservations:
1. For most of the jobs, the salaries are given annually. Meanwhile, there are also some jobs which have hourly salaries. Only a few of those jobs have daily salaries.
2. For salaries calculated annually, it has approximately right-skewed normal distribution.
3. For salaries calculated daily, there is no specific pattern regarding the distribution. Some jobs have relatively low daily salaries, while others have much higher salaries.
4. For salaries calculated hourly, most of them has a relatively low value, but there are still some jobs have relatively high hourly salaries.

```{r}
#converting salary on hourly scale anddaily scale to yearly scale
#no of working days in US in a year: 261 source: 
#no of working hours in US in a day: 8.4 hours 
job <- job %>% mutate(Annual_salary = if_else( Salary.Frequency == "Annual", round((Salary.Range.From + Salary.Range.To)/2,2),
                                 if_else(Salary.Frequency == "Daily", round((Salary.Range.From + Salary.Range.To)*261/2,2),
                                         round((Salary.Range.From + Salary.Range.To)*261*8.4/2,2))
                                 )
                               )

##make the list of category of each job id as a single observations 
df<-unnest(categoryList, cols = c(Job.Category))%>%
  mutate(Job.Category = trimws(Job.Category,"both"))%>%
  filter(Job.Category!="")

library(lubridate)
mdy(job$Posting.Date)

df_time<- df%>%
  merge(.,job[c("Job.ID","Posting.Date")], by = "Job.ID")%>%
  unique()%>%
  mutate(month = lubridate::month(mdy(Posting.Date)))%>%
  group_by(Job.Category,month)%>%
  mutate(count = n())


x<-df_time[1:50,]
library("ggridges")
library("tidyverse")

ggplot(x, aes(x=month,y=Job.Category,fill=Job.Category))+
  geom_density_ridges() + theme_ridges()+
  labs(x="month",y="count")+
  ggtitle("The counts of insects treated with different insecticides.")+
  theme(plot.title = element_text(hjust = 0.5))+
  theme(legend.position = "none")


ggplot(Orange, aes(x=circumference,y=Tree,fill = Tree))+
  geom_density_ridges(scale = 2, alpha=0.5) + theme_ridges()+
  scale_fill_brewer(palette = 4)+
  scale_y_discrete(expand = c(0.8, 0)) +
  scale_x_continuous(expand = c(0.01, 0))+
  labs(x="Circumference at Breast Height", y="Tree with ordering of max diameter")+
  ggtitle("Density estimation of circumference of different types of Trees")+
  theme(plot.title = element_text(hjust = 0.5))
## all job posting with only category, anuual salary and job id

df_popular<-df%>%
  filter(Job.Category %in% popular_category$Category)%>%
  merge(.,job[c("Job.ID","Annual_salary")], by = "Job.ID")%>%
  unique()

ggplot(df_popular,aes(x = reorder(Job.Category,Annual_salary,FUN=mean), y = Annual_salary)) +
geom_boxplot(color = "black", fill = "orange") +
ggtitle("Distribution of Salaries w.r.t Different Categories") + 
labs(x = "Category", y = "Anual Salary") + 
theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()




```



This boxplot gives us a general idea of the salary distribution for different kinds of jobs from the highest highest salary to lowest mean salary. For instance, we can see that jobs of Building Operations & Maintenance in general have lower salaries than those of Information Technology & Telecommunmications.

There are a lot of information in the job description and job requirements. Extracting the useful information from these features could be a challenging work.

In conclusion, these are all we have done right now. We identify the number of job postings with respect to different job categories. We also check the salaries of jobs with different salary frequency. Last but not least, we also inspect the salary range for different categories by drawing a boxplot. I think we are off a great start, and we will continue our analysis on the dataset from here.


# 3. Analyzing part

## 3.1 Word Cloud

Word Frequency and WordCloud from here:
```{r}
#library(SnowballC)
#library(wordcloud)
#library(RColorBrewer)
#set.seed(1234)
#wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          #max.words=200, random.order=FALSE, rot.per=0.35, 
          #colors=brewer.pal(8, "Dark2"))
```


## 3.2(Song's title, not decided yet)
```{r}
job <- job %>% mutate(Annual_salary_range = if_else( Salary.Frequency == "Annual", round(abs(Salary.Range.From - Salary.Range.To),2),
                                 if_else(Salary.Frequency == "Daily", round(abs(Salary.Range.From - Salary.Range.To)*261,2),
                                         round(abs(Salary.Range.From - Salary.Range.To)*261*8.4,2))
                                 )
                               )
```

```{r}
extractdf <- data.frame("id" = job$Job.ID, 
                        "agency" = job$Agency,
                        "posting_type" = job$Posting.Type,
                       "type" = job$Full.Time.Part.Time.indicator,
                       "num_positions" = job$X..Of.Positions,
                       "salary_annual_mean" = job$Annual_salary,
                       "salaray_annual_range" = job$Annual_salary_range)
```
Lookes like we have duplication in the table.  Delete these duplications.
```{r}
sum(duplicated(extractdf))

extractdf <- unique(extractdf) 
extractdf
```

Make sure each column in the property data type.
```{r}
extractdf$id <- as.integer(as.character(extractdf$id))
extractdf$num_positions <- as.integer(as.character(extractdf$num_positions))
extractdf$salary_annual_mean <- as.numeric(as.character(extractdf$salary_annual_mean))
extractdf$salaray_annual_range <- as.numeric(as.character(extractdf$salaray_annual_range))
```

```{r}
library(devtools)
devtools::install_github('madlogos/recharts')
library(recharts)
devtools::install_github('ramnathv/rCharts')
library(rCharts)
```

```{r}
mydf1 <- na.omit(extractdf)
head(mydf1)
```

```{r}
p <- nPlot(salaray_annual_range~salary_annual_mean,group='type',data=mydf1, type='scatterChart') 
p$yAxis(axisLabel = "Annual salary range")
p$xAxis(axisLabel = "Annual salary mean")
```

```{r}
options(RCHART_WIDTH = 700, RCHART_HEIGHT = 500)
rPlot(salaray_annual_range~salary_annual_mean | type, data = mydf1, color = 'type', type = 'point')
```

```{r}
echart(mydf1,~salary_annual_mean,~salaray_annual_range,series = ~type)
```


Besides, job category is also an importatnt feature, therefore we want to add it into this 'extractdf'.
```{r}
mydf = total <- merge(extractdf,df,by.x = "id",by.y = "Job.ID")
mydf
```
```{r}
summary(mydf$salary_annual_mean)
summary(mydf$salaray_annual_range)
```

```{r}
mydf$binned_salary_mean <- cut(mydf$salary_annual_mean,breaks = c(0,58588,78316,93434,218587))
mydf$binned_salary_range <- cut(mydf$salaray_annual_range,breaks = c(0,10000,28472,30447,160000))
```


## 3.3 Technology job posting trends

```{r}
tech_job = job[job$Job.Category == 'Technology, Data & Innovation', ]

tech_job %>%
  filter(!is.na(tech_job$Full.Time.Part.Time.indicator)) %>%
  ggplot(aes(Full.Time.Part.Time.indicator)) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Full time and part time")

```


```{r}
tech_job %>%
  filter(!is.na(tech_job$Salary.Frequency)) %>%
  ggplot(aes(Salary.Frequency)) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Salary frequency")
```


```{r}
tech_job %>%
  filter(!is.na(tech_job$Civil.Service.Title)) %>%
  ggplot(aes(fct_rev(fct_infreq(Civil.Service.Title)))) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Civil service title") + 
  coord_flip()
```


```{r}
tech_job %>%
  filter(!is.na(tech_job$Work.Location)) %>%
  ggplot(aes(fct_rev(fct_infreq(Work.Location)))) + 
  geom_bar(fill = 'lightblue') +
  labs(x = "Work location") + 
  coord_flip()
```


```{r}
tech_job %>%
  group_by(Civil.Service.Title) %>%
  summarise(mean_salary = mean(salary)) %>%
  ggplot(aes(x = reorder(Civil.Service.Title, mean_salary), y = mean_salary)) +
  geom_col(fill = 'lightblue') + 
  labs(x = 'Civil service title', 
       y = 'Average annual salary') + 
  coord_flip()
```

As a result, for technology job postings in New York City:
  
  - Full time jobs more than part time jobs.
  
  - Popular Location is Brooklyn.
  
  - Most wanted Civil Service is Computer Systems Manager.
  
  - Most jobs have over 60000 dollars average annual salary.
  
  


# 4. Conclusion


# 5. Future Works

(If we are provided more time, what we want to do?)



# 6. Requirements and References

## 6.1 Reqirements

### 6.1.1 File format
(You don't have to use the same format for this assignment -- PSet 5, part A -- and the final project itself.)

Choices are:

pdf_document  

html_document  

bookdown book: https://bookdown.org/yihui/bookdown/

shiny app: https://shiny.rstudio.com/  

(Remember that it's ok to have pieces of the project that don't fit into the chosen output format; in those cases you can provide links to the relevant material.)

    We decide to use the html_document as our output format.

### 6.1.2 Instructions
Be sure to review the final project instructions (https://edav.info/project.html), in particular the new section on reproducible workflow (https://edav.info/project.html#reproducible-workflow), which summarizes principles that we've discussed in class.


### 6.2 References
---
title: "EDAV Fall 2019 Final Project Group 23"
author: "Shaofeng Wu, Chengyou Ju, Mingrui Liu, Yujing Song"
output: html_document
---

```{r, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```

## I. Introduction

This project is created by Shaofeng Wu (sw3428), Chengyou Ju (cj2624), Mingrui Liu (ml4404), Yujing Song (ys3251) for STAT GR5702 Final Project Group 23.

As graduate students who are actively seeking jobs and internships, we come up with the idea that we can choose a topic related to the current employment situation in New York. Therefore, after doing some research, we decide to focus on studying the latest job postings in New York. We want to see, for instance, if there is a relationship between job categories and salaries, what skills are preferred by a particular kind of jobs, or if locations matter while choosing a job.


## II. Data sources

Our data come from https://data.cityofnewyork.us/City-Government/NYC-Jobs/kpav-sd4t, which contains current job postings available on the City of New Yorkâ€™s official jobs site http://www.nyc.gov/html/careers/html/search/search.shtml. 

Mingrui is responsible for looking for and collecting the data. Since the data can be directly exported from the website as a `.csv` file, there is no major obstacle for us when gathering the data. After downloading the data from the website, we use the built-in `read.csv` method to read and store the data as a data frame for future manipulation.

This data frame has 3040 oberservations of 28 variables. Each entry represents a job posting on the website, while the columns represent the information about each job, including Business Title, Salary Range, Job Description, etc.

The only problem about the data is that in the origin `.csv` file, there are plenty of empty entries. Therefore, when we read data from the file and store as a data frame, we fill all these blank entries with NAs.


## III. Data transformation

As mentioned in the previous section, since the dataset is stored in a neat `.csv` file, we only need to read the file using the `read.csv` method and store the data in the data frame `job`. Later, when we need some specific attributes of the job postings dataset, we can use simply `$` or pipes and the `%>%` operator to extract the columns we need.
```{r}
library(dplyr)
library(corpus)
library(ggplot2)
library(magrittr)
library(forcats)
library(tidyverse)
```

```{r}
job <- read.csv("data/raw/NYC_jobs.csv") # Read data from csv file
job[job == ''] <- NA # Set empty slots as NAs

## Remove duplicate jobs 
job <-job[!duplicated(job), ] %>%
  filter(!is.na(Job.Category))

dim(job) # Check the dimension of the data frame
```


## IV. Missing values

As mentioned above, we fill all empty slots in the original dataset with NAs, and we will keep these NAs in our data frame. Later, depends on our tasks, we will decide whether to drop the NAs or not.


## V. Results
```{r}
library(tm)
job_docs <- VCorpus(VectorSource(job)) # Whole dataset
# inspect(job_docs)
job_mini_req <- VCorpus(VectorSource(job$Minimum.Qual.Requirements)) # Minimum Qual Requirements
# inspect(job_mini_req)
job_pref_skil <- VCorpus(VectorSource(job$Preferred.Skills)) # Preferred Skills
# inspect(job_pref_skil)
```
```{r}
# Tech Jobs
tech_jobs <- subset(job, Job.Category == "Information Technology & Telecommunications")
dim(tech_jobs) # There should be 28 jobs related to technology
job_tech_mini_req <- VCorpus(VectorSource(tech_jobs$Minimum.Qual.Requirements))
# inspect(job_tech_mini_req)
job_tech_pref_skil <- VCorpus(VectorSource(tech_jobs$Preferred.Skills))
# inspect(job_tech_pref_skil)
```
```{r}
# All Jobs cross Minimum Qual Requirements
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_mini_req <- tm_map(job_mini_req, toSpace, "/")
# job_mini_req <- tm_map(job_mini_req, toSpace, "@")
# job_mini_req <- tm_map(job_mini_req, toSpace, "\\|")
job_mini_req <- tm_map(job_mini_req, content_transformer(tolower))
job_mini_req <- tm_map(job_mini_req, removeNumbers)
job_mini_req <- tm_map(job_mini_req, removeWords, stopwords("english"))
job_mini_req <- tm_map(job_mini_req, removeWords, c("the", "one", "two", "for", "must")) 
job_mini_req <- tm_map(job_mini_req, removePunctuation)
job_mini_req <- tm_map(job_mini_req, stripWhitespace)
# job_mini_req <- tm_map(job_mini_req, stemDocument)
```
```{r}
mini_req_matrix <- TermDocumentMatrix(job_mini_req)
mini_freq_m <- as.matrix(mini_req_matrix)
mini_freq_v <- sort(rowSums(mini_freq_m), decreasing=TRUE)
mini_freq <- data.frame(word = names(mini_freq_v), freq=mini_freq_v)
head(mini_freq, 20)
```
```{r}
library(wordcloud2)
wordcloud2(data = mini_freq)
```

```{r}
# All Jobs cross Preferred Skills
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_pref_skil <- tm_map(job_pref_skil, toSpace, "/")
# job_pref_skil <- tm_map(job_pref_skil, toSpace, "@")
# job_pref_skil <- tm_map(job_pref_skil, toSpace, "\\|")
job_pref_skil <- tm_map(job_pref_skil, content_transformer(tolower))
job_pref_skil <- tm_map(job_pref_skil, removeNumbers)
job_pref_skil <- tm_map(job_pref_skil, removeWords, stopwords("english"))
job_pref_skil <- tm_map(job_pref_skil, removeWords, c("the", "one", "two", "for", "must")) 
job_pref_skil <- tm_map(job_pref_skil, removePunctuation)
job_pref_skil <- tm_map(job_pref_skil, stripWhitespace)
# job_pref_skil <- tm_map(job_pref_skil, stemDocument)
```
```{r}
pref_skil_matrix <- TermDocumentMatrix(job_pref_skil)
pref_freq_m <- as.matrix(pref_skil_matrix)
pref_freq_v <- sort(rowSums(pref_freq_m), decreasing=TRUE)
pref_freq <- data.frame(word = names(pref_freq_v), freq=pref_freq_v)
pref_freq <- pref_freq[-1,]
head(pref_freq, 20)
```
```{r}
wordcloud2(data = pref_freq)
```

```{r}
# Tech Jobs cross Minimum Qual Requirements
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_tech_mini_req <- tm_map(job_tech_mini_req, toSpace, "/")
# job_tech_mini_req <- tm_map(job_tech_mini_req, toSpace, "@")
# job_tech_mini_req <- tm_map(job_tech_mini_req, toSpace, "\\|")
job_tech_mini_req <- tm_map(job_tech_mini_req, content_transformer(tolower))
job_tech_mini_req <- tm_map(job_tech_mini_req, removeNumbers)
job_tech_mini_req <- tm_map(job_tech_mini_req, removeWords, stopwords("english"))
job_tech_mini_req <- tm_map(job_tech_mini_req, removeWords, c("the", "one", "two", "for", "must")) 
job_tech_mini_req <- tm_map(job_tech_mini_req, removePunctuation)
job_tech_mini_req <- tm_map(job_tech_mini_req, stripWhitespace)
# job_tech_mini_req <- tm_map(job_tech_mini_req, stemDocument)
```
```{r}
tech_mini_matrix <- TermDocumentMatrix(job_tech_mini_req)
tech_mini_freq_m <- as.matrix(tech_mini_matrix)
tech_mini_freq_v <- sort(rowSums(tech_mini_freq_m), decreasing=TRUE)
tech_mini_freq <- data.frame(word = names(tech_mini_freq_v), freq=tech_mini_freq_v)
head(tech_mini_freq, 20)
```
```{r}
wordcloud2(data = tech_mini_freq)
```

```{r}
# Tech Jobs cross Preferred Skills
# toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, toSpace, "/")
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, toSpace, "@")
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, toSpace, "\\|")
job_tech_pref_skil <- tm_map(job_tech_pref_skil, content_transformer(tolower))
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removeNumbers)
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removeWords, stopwords("english"))
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removeWords, c("the", "one", "two", "for", "must")) 
job_tech_pref_skil <- tm_map(job_tech_pref_skil, removePunctuation)
job_tech_pref_skil <- tm_map(job_tech_pref_skil, stripWhitespace)
# job_tech_pref_skil <- tm_map(job_tech_pref_skil, stemDocument)
```
```{r}
tech_pref_matrix <- TermDocumentMatrix(job_tech_pref_skil)
tech_pref_freq_m <- as.matrix(tech_pref_matrix)
tech_pref_freq_v <- sort(rowSums(tech_pref_freq_m), decreasing=TRUE)
tech_pref_freq <- data.frame(word = names(tech_pref_freq_v), freq=tech_pref_freq_v)
head(tech_pref_freq, 20)
```
```{r}
wordcloud2(data = tech_pref_freq)
```


## VI. Interactive component


## VII. Conclusion

b) How do you plan to divide up the work? (Grading is on a group basis. The point of asking is to encourage you to think about this.)
To

    Chengyou may will work on the topic of salary vs. agency, and he may also use the word cloud to explore the relationship between these two variables. 
    
    Shaofeng will work on the data cleaning and mabipulation, and he will try to find connections among NAs or may come up with interesting topics when doing the data cleaning. 
    
    Yujing will investigate on the topic of salary vs. regions, and she also wants to draw a map to show the salary's region distribution.
    
    Mingrui will focus on the topic of salary vs. job category. Using different groups of job category to classify different levels of salaries may be something could be explored.
    
### 1.2 The Problem Statement

List three questions that you hope you will be able to answer from your research.

  Main topic: What factors maybe influence salary?

    a) The relationship between salary vs. regions.

    b) The relationship between salary vs. job category.

    c) The relationship between salary vs. agency.



# 2. Data Exploration

### 2.1 The Data Discription





### 2.2 Short Summary of Initial Investigations


```{r}
categoryList <- job %>%
  filter(!is.na(Job.Category))%>%
  select(Job.Category, Job.ID) %>%
  mutate(Job.Category = as.character(Job.Category),
         Job.Category = str_split(Job.Category, ",|&|,&"))

popular_category <-
  as.data.frame(unlist(categoryList["Job.Category"],use.names=FALSE))%>%
  set_colnames("Category")%>%
  mutate(Category = trimws(Category,"both")) %>%
  filter(!is.na(Category))%>%
  filter(Category !="")%>%
  filter(is.character(Category ))  %>%
  group_by(Category) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  slice(1:25)
  
ggplot(popular_category,aes(x = fct_reorder(Category,count), y = count))+
  geom_col(color = "black", fill = "orange")+
  ggtitle("Job Count by Category") +
  labs(x = "Category", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5)) + 
  coord_flip()
```

```{r}
job <- job%>%
  mutate(salary = Salary.Range.From+(Salary.Range.To-Salary.Range.From)/2)

Annual = job[job$Salary.Frequency=="Annual",]
ggplot(Annual, aes(salary)) +
  geom_histogram(bins = 40, color = "black", fill = "orange") +
  ggtitle("Salary Distribution (Annual)") + 
  labs(x = "Salary Range", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
Daily = job[job$Salary.Frequency=="Daily",]
ggplot(data = Daily, aes(Daily$salary)) +
  geom_histogram(bins = 40, color = "black", fill = "orange") +
  ggtitle("Salary Distribution (Daily)") + 
  labs(x = "Salary Range", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
Hourly = job[job$Salary.Frequency=="Hourly",]
ggplot(data = Hourly, aes(Hourly$salary)) +
  geom_histogram(bins = 40, color = "black", fill = "orange") +
  ggtitle("Salary Distribution (Hourly)") + 
  labs(x = "Salary Range", y = "Count") + 
  theme(plot.title = element_text(hjust = 0.5))
```

From those three plots above, we can have the following obeservations:
1. For most of the jobs, the salaries are given annually. Meanwhile, there are also some jobs which have hourly salaries. Only a few of those jobs have daily salaries.
2. For salaries calculated annually, it has approximately right-skewed normal distribution.
3. For salaries calculated daily, there is no specific pattern regarding the distribution. Some jobs have relatively low daily salaries, while others have much higher salaries.
4. For salaries calculated hourly, most of them has a relatively low value, but there are still some jobs have relatively high hourly salaries.

```{r}
#converting salary on hourly scale anddaily scale to yearly scale
#no of working days in US in a year: 261 source: 
#no of working hours in US in a day: 8.4 hours 
job <- job %>% mutate(Annual_salary = if_else( Salary.Frequency == "Annual", round((Salary.Range.From + Salary.Range.To)/2,2),
                                 if_else(Salary.Frequency == "Daily", round((Salary.Range.From + Salary.Range.To)*261/2,2),
                                         round((Salary.Range.From + Salary.Range.To)*261*8.4/2,2))
                                 )
                               )
##make the list of category of each job id as a single observations 
df <- data.frame( Job.ID= numeric(), category = character())
for(i in 1:dim(categoryList)[1]){
  cate <- unlist(categoryList[i,1])
  for (j in 1:length(cate)){
    single <- trimws(cate[j],"both")
    if( !is.na(single)){
      df <- rbind(df, data.frame(Job.ID = categoryList[i,2], category = single))
    }
  }
}
## all job posting with only category, anuual salary and job id
cate_salary_job <-job%>%
  select(Job.ID,Annual_salary)
##job category and salary
df2<-df%>%
  filter(category!="")%>%
  filter(category %in% popular_category$Category)%>%
  merge(.,cate_salary_job, by = "Job.ID")%>%
  unique()
ggplot(df2,aes(x = reorder(category,Annual_salary,FUN=mean), y = Annual_salary)) +
geom_boxplot(color = "black", fill = "orange") +
ggtitle("Distribution of Salaries w.r.t Different Categories") + 
labs(x = "Category", y = "Anual Salary") + 
theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()
```



This boxplot gives us a general idea of the salary distribution for different kinds of jobs from the highest highest salary to lowest mean salary. For instance, we can see that jobs of Building Operations & Maintenance in general have lower salaries than those of Information Technology & Telecommunmications.

There are a lot of information in the job description and job requirements. Extracting the useful information from these features could be a challenging work.

In conclusion, these are all we have done right now. We identify the number of job postings with respect to different job categories. We also check the salaries of jobs with different salary frequency. Last but not least, we also inspect the salary range for different categories by drawing a boxplot. I think we are off a great start, and we will continue our analysis on the dataset from here.


# 3. Analyzing part

## 3.1 Word Cloud

Word Frequency and WordCloud from here:
```{r}
library(tm)
library(SnowballC)
library(wordcloud)
library(wordcloud2)
library(RColorBrewer)
mini <- Corpus(VectorSource(job$Minimum.Qual.Requirements))
# inspect(mini)
```
```{r}
toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
mini <- tm_map(mini, toSpace, "/")
mini <- tm_map(mini, toSpace, "@")
mini <- tm_map(mini, toSpace, "\\|")
mini <- tm_map(mini, content_transformer(tolower))
```
```{r}
# Remove numbers
mini <- tm_map(mini, removeNumbers)
# Remove english common stopwords
mini <- tm_map(mini, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
mini <- tm_map(mini, removeWords, c("blabla1", "blabla2")) 
# Remove punctuations
mini <- tm_map(mini, removePunctuation)
# Eliminate extra white spaces
mini <- tm_map(mini, stripWhitespace)
# Text stemming
mini <- tm_map(mini, stemDocument)
```
```{r}
dtm <- TermDocumentMatrix(mini)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing=TRUE)
d <- data.frame(word = names(v), freq=v)
head(d, 10)
```
```{r}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```
```{r}
wordcloud2(data = d)
```


## 3.2 Region distribution of salaries

# 4. Conclusion


# 5. Future Works

(If we are provided more time, what we want to do?)



# 6. Requirements and References

## 6.1 Reqirements

### 6.1.1 File format
(You don't have to use the same format for this assignment -- PSet 5, part A -- and the final project itself.)

Choices are:

pdf_document  

html_document  

bookdown book: https://bookdown.org/yihui/bookdown/

shiny app: https://shiny.rstudio.com/  

(Remember that it's ok to have pieces of the project that don't fit into the chosen output format; in those cases you can provide links to the relevant material.)

    We decide to use the html_document as our output format.

### 6.1.2 Instructions
Be sure to review the final project instructions (https://edav.info/project.html), in particular the new section on reproducible workflow (https://edav.info/project.html#reproducible-workflow), which summarizes principles that we've discussed in class.


### 6.2 References